{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "differential-justice",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator,load_img\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from keras.layers import BatchNormalization,GlobalAveragePooling2D,Conv2D,MaxPooling2D,Flatten,Dense,Dropout\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "periodic-cardiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height, img_width=(225,225)\n",
    "batch_size=32\n",
    "#train,validation and test dataset\n",
    "train_data_dir=\"/home/qwevo/CONDAPROJECTS/ip102_v1.1/ip102_v1.1/proccesed/images/train\"\n",
    "test_data_dir=\"/home/qwevo/CONDAPROJECTS/ip102_v1.1/ip102_v1.1/proccesed/images/test\"\n",
    "valid_data_dir=\"/home/qwevo/CONDAPROJECTS/ip102_v1.1/ip102_v1.1/proccesed/images/val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "first-relationship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3240 images belonging to 18 classes.\n",
      "Found 720 images belonging to 18 classes.\n"
     ]
    }
   ],
   "source": [
    "#Keras ImageDataGenerator class makes image augmentation and labeling much easier\n",
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.4)\n",
    "#create the training dataset\n",
    "train_generator =train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height,img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training')\n",
    "#creating validation dataset\n",
    "valid_generator =train_datagen.flow_from_directory(\n",
    "    valid_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "surgical-profession",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1080 images belonging to 18 classes.\n"
     ]
    }
   ],
   "source": [
    "#creating the testdataset\n",
    "test_generator =train_datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=(img_height,img_width),\n",
    "    batch_size=1,\n",
    "    class_mode='categorical',\n",
    "    subset='training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "boring-shipping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 225, 225, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x,y=test_generator.next()\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dominican-daniel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 225, 225, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y=train_generator.next()\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sustainable-mustang",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#classes on the training set\n",
    "train_generator.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dietary-structure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#classes on the test set\n",
    "test_generator.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "rural-grenada",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the labels \n",
    "labels = '\\n'.join(sorted(train_generator.class_indices.keys()))\n",
    "with open('labels.txt', 'w') as f:\n",
    " f.write(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "pediatric-checkout",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "102/102 [==============================] - 605s 6s/step - loss: 1.8755 - accuracy: 0.4558 - val_loss: 0.9103 - val_accuracy: 0.7347\n",
      "Epoch 2/5\n",
      "102/102 [==============================] - 646s 6s/step - loss: 0.4439 - accuracy: 0.8658 - val_loss: 0.8521 - val_accuracy: 0.7486\n",
      "Epoch 3/5\n",
      "102/102 [==============================] - 11530s 114s/step - loss: 0.3016 - accuracy: 0.9111 - val_loss: 0.8426 - val_accuracy: 0.7458\n",
      "Epoch 4/5\n",
      "102/102 [==============================] - 478s 5s/step - loss: 0.2174 - accuracy: 0.9396 - val_loss: 0.7842 - val_accuracy: 0.7708\n",
      "Epoch 5/5\n",
      "102/102 [==============================] - 458s 4s/step - loss: 0.1870 - accuracy: 0.9460 - val_loss: 0.7235 - val_accuracy: 0.7736\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f405874b580>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Name =\"ip102res50={}\".format(int(time.time()))\n",
    "tensorboard = TensorBoard(log_dir=\"ip102res50logs/{}\".format(Name))\n",
    "base_model=ResNet50(include_top=False, weights='imagenet')\n",
    "#base_model.load_weights('resnet50_weights_tf_dim_ordering_tf_kernels.h5')\n",
    "x=base_model.output\n",
    "#Add dense & Pooling Layer\n",
    "x=GlobalAveragePooling2D()(x)\n",
    "x=Dense(64,activation=\"relu\")(x)\n",
    "predictions=Dense(train_generator.num_classes,activation=\"sigmoid\")(x)\n",
    "model=Model(inputs=base_model.input,outputs=predictions)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable=False\n",
    "\n",
    "#Compile the model    \n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#train the model for 5 epochs\n",
    "model.fit(train_generator,epochs=5,validation_data = valid_generator,callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "radical-syndrome",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 372s 4s/step - loss: 0.1315 - accuracy: 0.9633\n",
      "Train Set loss, accuracy respectively: [0.13148875534534454, 0.9632716178894043]\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Set loss, accuracy respectively: \" + str(model.evaluate(train_generator)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "moved-somerset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 84s 4s/step - loss: 0.7222 - accuracy: 0.7778\n",
      "Validation Set loss, accuracy respectively: [0.7222334146499634, 0.7777777910232544]\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation Set loss, accuracy respectively: \" + str(model.evaluate(valid_generator)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "portuguese-quest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1080/1080 [==============================] - 286s 264ms/step - loss: 0.3955 - accuracy: 0.8759\n",
      "Test Set loss, accuracy respectively: [0.3954898416996002, 0.8759258985519409]\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Set loss, accuracy respectively: \" + str(model.evaluate(test_generator)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adolescent-trustee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('ip102resnet50.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "entertaining-kitty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: assets\n"
     ]
    }
   ],
   "source": [
    "# Saving the model\n",
    "import tensorflow as tf\n",
    "saved_model_dir = ''\n",
    "tf.saved_model.save(model, saved_model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "italian-egypt",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the model to tflitemodel\n",
    "import tensorflow as tf\n",
    "converter =tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "tflite_model = converter.convert()\n",
    "with open('bugres50model.tflite', 'wb') as f:\n",
    " f.write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "loaded-theater",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: [1 1 1 3]\n",
      "input type: <class 'numpy.float32'>\n",
      "output shape: [ 1 18]\n",
      "output type: <class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "#check the tflite model input details\n",
    "import tensorflow as tf\n",
    "interpreter=tf.lite.Interpreter(model_path=\"bugres50model.tflite\")\n",
    "input_details=interpreter.get_input_details()\n",
    "output_details=interpreter.get_output_details()\n",
    "print(\"input shape:\",input_details[0]['shape'])\n",
    "print(\"input type:\",input_details[0]['dtype'])\n",
    "print(\"output shape:\",output_details[0]['shape'])\n",
    "print(\"output type:\",output_details[0]['dtype'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "latter-commonwealth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: [32  1  1  3]\n",
      "input type: <class 'numpy.float32'>\n",
      "output shape: [32 18]\n",
      "output type: <class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "#change the input \n",
    "interpreter.resize_tensor_input(input_details[0]['index'],(32,1,1,3))\n",
    "interpreter.resize_tensor_input(output_details[0]['index'],(32,18))\n",
    "interpreter.allocate_tensors()\n",
    "input_details=interpreter.get_input_details()\n",
    "output_details=interpreter.get_output_details()\n",
    "print(\"input shape:\",input_details[0]['shape'])\n",
    "print(\"input type:\",input_details[0]['dtype'])\n",
    "print(\"output shape:\",output_details[0]['shape'])\n",
    "print(\"output type:\",output_details[0]['dtype'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "collective-thumb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'float32'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_generator.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "extensive-elimination",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y=test_generator.next()\n",
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "rapid-listing",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "SavedModel file does not exist at: try_pest_resnet50.h5/{saved_model.pbtxt|saved_model.pb}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-3145e0d87172>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'try_pest_resnet50.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mnb_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    209\u001b[0m       \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mloader_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m    109\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot parse file %s: %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpath_to_pbtxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     raise IOError(\"SavedModel file does not exist at: %s/{%s|%s}\" %\n\u001b[0m\u001b[1;32m    112\u001b[0m                   (export_dir,\n\u001b[1;32m    113\u001b[0m                    \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVED_MODEL_FILENAME_PBTXT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: SavedModel file does not exist at: try_pest_resnet50.h5/{saved_model.pbtxt|saved_model.pb}"
     ]
    }
   ],
   "source": [
    "#confusion matric\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import tensorflow as tf\n",
    "\n",
    "model =tf.keras.models.load_model('try_pest_resnet50.h5')\n",
    "filenames=test_generator.filenames\n",
    "nb_samples=len(test_generator)\n",
    "y_prob=[]\n",
    "y_act=[]\n",
    "test_generator.reset()\n",
    "for _ in range(nb_samples):\n",
    "    X_test,Y_test=test_generator.next()\n",
    "    y_prob.append(model.predict(X_test))\n",
    "    y_act.append(Y_test)\n",
    "\n",
    "predicted_class=[list(train_generator.class_indices.keys())[i.argmax()]for i in y_prob]\n",
    "actual_class=[list(train_generator.class_indices.keys())[i.argmax()]for i in y_act]\n",
    "    \n",
    "out_df=pd.DataFrame(np.vstack([predicted_class,actual_class]).T,columns=['predicted_class','actual_class'])\n",
    "confusion_matrix=pd.crosstab(out_df['actual_class'],out_df['predicted_class'],rownames=['actual'],colnames=['predicted'])\n",
    "\n",
    "sn.heatmap(confusion_matrix,cmap='Blues',annot=True,fmt='d')\n",
    "plt.show()\n",
    "print('test_accuracy : {}'.format((np.diagonal(confusion_matrix).sum()/confusion_matrix.sum().sum()*100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "excess-convention",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/1/assets\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "export_dir = 'saved_model/1'\n",
    "tf.saved_model.save(model, export_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1fbfd645",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Select mode of optimization\n",
    "mode = \"Storage\" #@param [\"Default\", \"Storage\", \"Speed\"]\n",
    "\n",
    "if mode == 'Storage':\n",
    "  optimization = tf.lite.Optimize.OPTIMIZE_FOR_SIZE\n",
    "elif mode == 'Speed':\n",
    "  optimization = tf.lite.Optimize.OPTIMIZE_FOR_LATENCY\n",
    "else:\n",
    "  optimization = tf.lite.Optimize.DEFAULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d5509b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)\n",
    "#converter.optimizations = []\n",
    "converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "#converter.allow_custom_ops=False\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "479b5f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_model_file = 'model.tflite'\n",
    "\n",
    "with open(tflite_model_file, \"wb\") as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4cecb4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "output_index = interpreter.get_output_details()[0][\"index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e929bbb",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.3) /tmp/pip-req-build-afu9cjzs/opencv/modules/imgproc/src/resize.cpp:4051: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-b82f0905a12b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mimg_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/qwevo/CONDAPROJECTS/ip102_v1.1/ip102_v1.1/proccesed/images/test/army worm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mimg_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m225\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m225\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mimg_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_array\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimg_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.5.3) /tmp/pip-req-build-afu9cjzs/opencv/modules/imgproc/src/resize.cpp:4051: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "img_array = cv2.imread('/home/qwevo/CONDAPROJECTS/ip102_v1.1/ip102_v1.1/proccesed/images/test/army worm')\n",
    "img_array = cv2.resize(img_array,(225,225))\n",
    "img_array = img_array/255\n",
    "img_array = np.expand_dims(img_array,axis=0)\n",
    "img = tf.cast(img_array, tf.float32)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e1d251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather results for the randomly sampled test images\n",
    "\n",
    "predictions = []\n",
    "\n",
    "test_labels = []\n",
    "\n",
    "test_images = []\n",
    "\n",
    "interpreter.set_tensor(input_index, img)\n",
    "\n",
    "interpreter.invoke()\n",
    "\n",
    "predictions.append(interpreter.get_tensor(output_index))\n",
    "\n",
    "#test_labels.append(label[0])\n",
    "\n",
    "#test_images.append(np.array(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77284d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18774ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.argmax(predictions)\n",
    "print('Predicted crop is: ',class_names[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d4e319",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
